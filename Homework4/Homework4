

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn import metrics
from scipy import stats
import seaborn as sns; sns.set()

breast = load_breast_cancer()
breast_data = breast.data
breast_data.shape

breast_input = pd.DataFrame(breast_data)
breast_input.head()

breast_labels = breast.target

breast_labels.shape

labels = np.reshape(breast_labels,(569,1))

final_breast_data = np.concatenate([breast_data,labels], axis = 1)

final_breast_data.shape

breast_dataset = pd.DataFrame(final_breast_data)
features = breast.feature_names
features

features_labels = np.append(features, 'label')

breast_dataset.columns = features_labels

breast_dataset.head()

breast_dataset['label'].replace(0, 'Benign',inplace=True) 
breast_dataset['label'].replace(1, 'Malignant',inplace=True) 

breast_dataset.tail() 

def binary_map(x):
    return x.map({'Benign': 0, 'Malignant': 1})

breast_dataset['label'] = breast_dataset['label'].map({'Benign': 0, 'Malignant': 1})

breast_dataset.tail()

X = breast_dataset.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]]
Y = breast_dataset.iloc[:,30]
from sklearn.preprocessing import StandardScaler
X = StandardScaler().fit_transform(X)

#Splits the data
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = .2,random_state = 0)

Accuracy, Precision, Recall, size = list(),list(),list(),list()


from sklearn.svm import SVC
model = SVC(kernel = 'linear')

from sklearn.decomposition import PCA
Accuracy.clear()
Precision.clear()
Recall.clear()
size.clear()

for n in range(1,25):
    pca = PCA(n_components = n)
    pca.fit(X_train)
    train_X = pca.transform(X_train)
    test_X = pca.transform(X_test)
    model.fit(train_X, Y_train)
    Y_pred = model.predict(test_X)
    Accuracy.append(metrics.accuracy_score(Y_test,Y_pred))
    Precision.append(metrics.precision_score(Y_test, Y_pred))
    Recall.append(metrics.recall_score(Y_test, Y_pred))
    size.append(n)

Accuracy

fig = plt.figure(figsize = (8,6)) 
ax = fig.add_subplot(1,1,1)  
ax.set_xlabel('Number of Components Ks')
ax.set_title('Classification Accuracy, Precision, and Recall')
ax.scatter(size,Accuracy[:],marker = '+', label = 'Accuracy')
ax.scatter(size,Recall, marker ='o', label = 'Recall')
ax.scatter(size,Precision, marker ='*', label = 'Precision')
ax.legend()

#It can be noticed that in the logistic regression from homework 3 the accuracy, precision, and recall don't change after the number of components hits 12. In this case the accuracy and recall can become even higher by increasing the number of components until about the 22nd component. The precision will stay the same.

model = SVC(kernel = 'poly')
Accuracy.clear()
Precision.clear()
Recall.clear()
size.clear()

for n in range(1,25):
    pca = PCA(n_components = n)
    pca.fit(X_train)
    train_X = pca.transform(X_train)
    test_X = pca.transform(X_test)
    model.fit(train_X, Y_train)
    Y_pred = model.predict(test_X)
    Accuracy.append(metrics.accuracy_score(Y_test,Y_pred))
    Precision.append(metrics.precision_score(Y_test, Y_pred))
    Recall.append(metrics.recall_score(Y_test, Y_pred))
    size.append(n)

fig = plt.figure(figsize = (8,6)) 
ax = fig.add_subplot(1,1,1)  
ax.set_xlabel('Number of Components Ks')
ax.set_title('Classification Accuracy, Precision, and Recall of poly')
ax.scatter(size,Accuracy[:],marker = '+', label = 'Accuracy')
ax.scatter(size,Recall, marker ='o', label = 'Recall')
ax.scatter(size,Precision, marker ='*', label = 'Precision')
ax.legend()

model = SVC(kernel = 'rbf')
Accuracy.clear()
Precision.clear()
Recall.clear()
size.clear()

for n in range(1,25):
    pca = PCA(n_components = n)
    pca.fit(X_train)
    train_X = pca.transform(X_train)
    test_X = pca.transform(X_test)
    model.fit(train_X, Y_train)
    Y_pred = model.predict(test_X)
    Accuracy.append(metrics.accuracy_score(Y_test,Y_pred))
    Precision.append(metrics.precision_score(Y_test, Y_pred))
    Recall.append(metrics.recall_score(Y_test, Y_pred))
    size.append(n)

fig = plt.figure(figsize = (8,6)) 
ax = fig.add_subplot(1,1,1)  
ax.set_xlabel('Number of Components Ks')
ax.set_title('Classification Accuracy, Precision, and Recall of rbf')
ax.scatter(size,Accuracy[:],marker = '+', label = 'Accuracy')
ax.scatter(size,Recall, marker ='o', label = 'Recall')
ax.scatter(size,Precision, marker ='*', label = 'Precision')
ax.legend()

model = SVC(kernel = 'sigmoid')
Accuracy.clear()
Precision.clear()
Recall.clear()
size.clear()

for n in range(1,25):
    pca = PCA(n_components = n)
    pca.fit(X_train)
    train_X = pca.transform(X_train)
    test_X = pca.transform(X_test)
    model.fit(train_X, Y_train)
    Y_pred = model.predict(test_X)
    Accuracy.append(metrics.accuracy_score(Y_test,Y_pred))
    Precision.append(metrics.precision_score(Y_test, Y_pred))
    Recall.append(metrics.recall_score(Y_test, Y_pred))
    size.append(n)

fig = plt.figure(figsize = (8,6)) 
ax = fig.add_subplot(1,1,1)  
ax.set_xlabel('Number of Components Ks')
ax.set_title('Classification Accuracy, Precision, and Recall of sigmoid')
ax.scatter(size,Accuracy[:],marker = '+', label = 'Accuracy')
ax.scatter(size,Recall, marker ='o', label = 'Recall')
ax.scatter(size,Precision, marker ='*', label = 'Precision')
ax.legend()

# 

#The highest accuracy that could be obtained was with the rbf model with 19 components. In the poly kernel the classification accuracy, recall, and
#precision appear to not change much with the change of the components. The sigmoid kernel appears to have the lowest classification accuracies besides the poly kernel. The kernels appear to have the similar trend that the accuracy, precision, and recall increase with the number of components.

# Problem 2 

housing = pd.DataFrame(pd.read_csv('https://raw.githubusercontent.com/rrusse33/ECGR4105/main/Homework1/Housing.csv'))
housing.head()

varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning','prefarea']
def binary_map(x):
    return x.map({'yes':1,'no': 0})

housing[varlist] = housing[varlist].apply(binary_map)
housing.head()

X = housing.iloc[:,[1,2,3,4,5,6,7,8,9,10,11]]
Y = housing.iloc[:,0]

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = .2,random_state = 0)

from sklearn.svm import SVR
svr_lin = SVR(kernel = 'linear')

pca = PCA(n_components = 1)
pca.fit(X_train)
train_X1 = pca.transform(X_train)
test_X1 = pca.transform(X_test)


test_X1

svr_lin.fit(train_X1, Y_train)

y_lin= svr_lin.predict(test_X1)

plt.plot(test_X1, y_lin, color='c', lw=1, label='Linear model') 
plt.scatter(test_X1, Y_test, color = 'darkorange', label = 'data')
plt.xlabel('PCA extracted data') 
plt.ylabel('price') 
plt.title('Support Vector Regression') 
plt.legend() 
plt.show() 

svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1) 
svr_sigmoid = SVR(kernel='sigmoid') 

y_rbf = svr_rbf.fit(train_X1, Y_train).predict(test_X1)

#When compared to the linear regression model from homework 1 the linear regression model appears more scaled in this case. Through feature extraction the data appears to go more into the negatives which makes it harder to read. The data was split differently in homework 1 than that of this. Homework 1 had a 70% and 30% split between the training and test sets.



y_sig = svr_sigmoid.fit(train_X1, Y_train).predict(test_X1)

plt.scatter(test_X1, Y_test, color = 'darkorange', label = 'data')
plt.plot(test_X1, y_rbf, color='navy', lw=2, label='RBF model') 
plt.xlabel('PCA extracted data') 
plt.ylabel('price') 
plt.title('Support Vector Regression') 
plt.legend() 
plt.show() 

plt.scatter(test_X1, Y_test, color = 'darkorange', label = 'data')
plt.plot(test_X1, y_sig, color='cornflowerblue', lw=2, label='Sigmoid model') 
plt.xlabel('PCA extracted data') 
plt.ylabel('price') 
plt.title('Support Vector Regression') 
plt.legend() 
plt.show() 

#The others kernels appear to do very little and appear the same for each. The other kernels seem to not be very accurate when compared to the linear kernel. The regression models appear as a flat horizontal line which means nothing is really captured from the data.

