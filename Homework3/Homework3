import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn import metrics

breast = load_breast_cancer()
breast_data = breast.data
breast_data.shape

breast_input = pd.DataFrame(breast_data)
breast_input.head()

breast_labels = breast.target

breast_labels.shape

labels = np.reshape(breast_labels,(569,1))

final_breast_data = np.concatenate([breast_data,labels], axis = 1)

final_breast_data.shape

breast_dataset = pd.DataFrame(final_breast_data)
features = breast.feature_names
features

features_labels = np.append(features, 'label')

breast_dataset.columns = features_labels

breast_dataset.head()

breast_dataset['label'].replace(0, 'Benign',inplace=True) 
breast_dataset['label'].replace(1, 'Malignant',inplace=True) 

breast_dataset.tail() 

def binary_map(x):
    return x.map({'Benign': 0, 'Malignant': 1})

breast_dataset['label'] = breast_dataset['label'].map({'Benign': 0, 'Malignant': 1})

breast_dataset.tail()

X = breast_dataset.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]]
Y = breast_dataset.iloc[:,30]
from sklearn.preprocessing import StandardScaler
X = StandardScaler().fit_transform(X)

#Splits the data
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = .2,random_state = 0)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0,max_iter = 10000)
classifier.fit(X_train, Y_train)

Y_pred = classifier.predict(X_test)
Y_pred

print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred)) 
print("Precision:",metrics.precision_score(Y_test, Y_pred)) 
print("Recall:",metrics.recall_score(Y_test, Y_pred)) 

from sklearn.metrics import confusion_matrix
cnf_matrix = confusion_matrix(Y_test,Y_pred)
import seaborn as sns 
class_names=[0,1] # name  of classes 
fig, ax = plt.subplots() 
tick_marks = np.arange(len(class_names)) 
plt.xticks(tick_marks, class_names) 
plt.yticks(tick_marks, class_names) 
# create heatmap 
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g') 
ax.xaxis.set_label_position("top") 
plt.tight_layout() 
plt.title('Confusion matrix', y=1.1) 
plt.ylabel('Actual label') 
plt.xlabel('Predicted label') 

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def compute_cost(X,y, theta):
    """
    Compute cost for logistic regression
    
    Input Parameters
    ----------------
    X: 2D array where each row represent the training example 
       and each column represents a training feature 
       m = number of training examples 
       n = number of features
    y: 1D array of target value for each training example. Dimension (1 x m)
    
    theta: 1D array of fitting parameters or weights. Dimension (1 x n)
    
    Output Parameters
    -----------------
    J : Scalar value.
    """
    m = X.shape[0]
    J = -(1/m) * np.sum(y*np.log(sigmoid(X * theta)).T + (1 - y) * np.log(1 - sigmoid(X * theta)))
    return J

def gradient_descent(X, y, theta, alpha, iterations, X_test, Y_test):
    """
    Compute cost for logistic regression
    
    Input Parameters
    ----------------
    X : 2D array where each row represent the training example 
        and each column represents a training feature 
        m = number of training examples 
        n = number of features
    y : 1D array of target value for each training example. Dimension (1 x m)
    theta : 1D array of fitting parameters or weights. Dimension (1 x n)
    alpha : Learning Rate Scaler value
    iterations : No of iterations. Scalar Value
    
    Output Parameters
    -----------------
    theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n) 
    cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)
    """
    m = X.shape[0]
    costs = np.zeros(iterations)
    accuracy = np.zeros(iterations)
    y = np.reshape(y.shape[0],1)
    for i in range(1, iterations):
        b = (sigmoid(np.dot(X, theta) )- y)
        theta = theta - (alpha/m) * np.matmul(np.transpose(X),b)
        #theta = theta - (alpha/m) * X.T * (sigmoid(X * theta) - y)
        Y_pred = predict(X_test, theta)
        accuracy[i] = metrics.accuracy_score(Y_test, Y_pred)
        cost = compute_cost(X[i,:], y, theta)
        costs[i] = cost
    return theta, costs, accuracy     
    

def predict(X, theta):
    
    a = sigmoid(np.matmul(X,theta))
    a [a >= .5] = 1
    a [a < .5] = 0
    return a

theta = np.zeros((30))
iterations = 30
alpha = .002
#Y_train = np.reshape(Y_train.shape[0],1)

thetas1, loss_history, accuracy_history = gradient_descent(X_train, Y_train, theta, alpha, iterations, X_test, Y_test)
loss_history

len(loss_history)

plt.plot(range(0, iterations), loss_history)
plt.xlabel('Number of Iterations')
plt.ylabel('Cost (J)')


plt.plot(range(0,iterations), accuracy_history)
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')

# Problem 2

pcaAccuracy, pcaPrecision, pcaRecall, size = list(), list(), list(), list()
train_X = X_train
train_Y = Y_train
test_X = X_test
pcaLog =  LogisticRegression(random_state = 0)


pcaAccuracy.clear()
pcaPrecision.clear()
pcaRecall.clear()
size.clear()
from sklearn.decomposition import PCA

for n in range(1,15):
    pca = PCA(n_components=n)
    pca.fit(X_train)
    train_X = pca.transform(X_train)
    test_X = pca.transform(X_test)
    pcaLog.fit(train_X, train_Y)
    pred_Y = pcaLog.predict(test_X)
    pcaAccuracy.append(metrics.accuracy_score(test_Y, pred_Y))
    pcaPrecision.append(metrics.precision_score(test_Y, pred_Y))
    pcaRecall.append(metrics.recall_score(Y_test, pred_Y))
    size.append(n)



pcaAccuracy

len(pcaAccuracy)

fig = plt.figure(figsize = (8,6)) 
ax = fig.add_subplot(1,1,1)  
ax.set_xlabel('Number of Components Ks')
ax.set_title('Classification Accuracy, Precision, and Recall')
ax.scatter(size,pcaAccuracy[:],marker = '+', label = 'Accuracy')
ax.scatter(size,pcaRecall, marker ='o', label = 'Recall')
ax.scatter(size,pcaPrecision, marker ='*', label = 'Precision')
ax.legend()


# Problem 3

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
ldaAccuracy, ldaPrecision, ldaRecall = list(), list(), list()
train_X = X_train
train_Y = Y_train
test_X = X_test

ldaAccuracy.clear()
ldaPrecision.clear()
ldaRecall.clear()
size.clear()


lda = LinearDiscriminantAnalysis(n_components = 1)
lda.fit(X_train, Y_train)



train_X = lda.transform(X_train)
#test_X = lda.transform(X_test)
pred_Y = lda.predict(test_X)
ldaAccuracy.append(metrics.accuracy_score(Y_test, pred_Y))
ldaPrecision.append(metrics.precision_score(Y_test, pred_Y))
ldaRecall.append(metrics.recall_score(Y_test, pred_Y))

size.append(n)

print("Accuracy:",metrics.accuracy_score(Y_test, pred_Y)) 
print("Precision:",metrics.precision_score(Y_test, pred_Y)) 
print("Recall:",metrics.recall_score(Y_test, pred_Y)) 



# Problem 4

ldaAccuracy, ldaPrecision, ldaRecall = list(), list(), list()
train_X = X_train
train_Y = Y_train
test_X = X_test

ldaAccuracy.clear()
ldaPrecision.clear()
ldaRecall.clear()
size.clear()

lda = LinearDiscriminantAnalysis(n_components=1)
lda.fit(X_train, Y_train)

train_X = lda.transform(X_train)
test_X = lda.transform(X_test)

ldaLog = LogisticRegression(random_state = 0)
ldaLog.fit(train_X,Y_train)

pred_Y = ldaLog.predict(test_X)

print("Accuracy:",metrics.accuracy_score(Y_test, pred_Y)) 
print("Precision:",metrics.precision_score(Y_test, pred_Y)) 
print("Recall:",metrics.recall_score(Y_test, pred_Y))

