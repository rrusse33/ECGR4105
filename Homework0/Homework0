import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 

df = pd.read_csv('https://raw.githubusercontent.com/rrusse33/ECGR4105/main/Homework0/D3.csv')
df.head()

X1= df.values[:, 0]
X2 = df.values[:, 1]
X3 = df.values[:, 2]
y = df.values[:, 3]
m = len(y)

plt.scatter(X1, y, color = 'red', marker = '+')
plt.grid()
plt.rcParams["figure.figsize"] = (10,6)
plt.xlabel('X1 of dataset')
plt.ylabel('y of dataset')
plt.title('Data set')

X_0 = np.ones((m,1))
X_1 = X1.reshape(m,1)
X = np.hstack((X_0, X_1))
X[:10]

theta = np.zeros(2)
theta

def compute_cost_function(X, y, theta):
    """
    Compute cost function for linear regression
    
    Input Parameters
    ----------------
    X : 2D array where each row is a training set and each column represent
        m = number of training sets
        n = number of featured X values
    y : 1D array of target values for each training set. dimension (1 x m)
    theta : 1D array of fitting parameters. Dimension (1 x n)
    
    Output Parameters
    -----------------
    J : Scalar value.
    """
    predictions = X.dot(theta)
    errors = np.subtract(predictions, y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    
    return J
    
def gradient_descent(X, y, theta, alpha, iterations):
    """
    Compute cost function for linear regression
    
    Input Parameters
    ----------------
    X : 2D array where each row is a training set and each column represent
        m = number of training sets
        n = number of featured X values
    y : 1D array of targets for each value of training sets. dimension(m x 1)
    theta : 1D array of fitting parameters. Dimension (1 x n)
    alpha : Learning rate. Scalar value
    iterations : No of iterations. Scalar value.
    
    Output Parameters
    -----------------
    theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)
    cost_history : 1D array that contains the cost for each iteration. Dimension (m x 1)
    """
    cost_history = np.zeros(iterations)
    
    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, y)
        sum_delta = (alpha / m) * X.transpose().dot(errors);
        theta = theta - sum_delta;
        cost_history[i] = compute_cost_function(X, y, theta)
        
    return theta, cost_history
    
iterations = 1500;
alpha = 0.01;

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)

plt.scatter(X[:,1], y, color = 'red', marker = '+', label = 'Training Data X1')
plt.plot(X[:,1], X.dot(theta), color = 'green', label = 'Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('X1 of data set')
plt.ylabel('y of data set')
plt.title('Linear Regression Fit')
plt.legend()

plt.plot(range(1, iterations  + 1), cost_history, color = 'blue')
plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

plt.scatter(X2, y, color = 'red', marker = '+')
plt.grid()
plt.rcParams["figure.figsize"] = (10,6)
plt.xlabel('X2 of dataset')
plt.ylabel('y of dataset')
plt.title('Data set for X2 and y')

X_2 = X2.reshape(m,1)
X = np.hstack((X_0, X_2))
X[:10]

theta = np.zeros(2)
theta

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)

plt.scatter(X[:,1], y, color = 'red', marker = '+', label = 'Training Data X2')
plt.plot(X[:,1], X.dot(theta), color = 'green', label = 'Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('X of data set')
plt.ylabel('y of data set')
plt.title('Linear Regression Fit')
plt.legend()

plt.plot(range(1, iterations  + 1), cost_history, color = 'blue')
plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

plt.scatter(X3, y, color = 'red', marker = '+')
plt.grid()
plt.rcParams["figure.figsize"] = (10,6)
plt.xlabel('X3 of dataset')
plt.ylabel('y of dataset')
plt.title('Data set for X3 and y')

X_3 = X3.reshape(m,1)
X = np.hstack((X_0, X_3))
X[:10]

theta = np.zeros(2)
theta

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)

plt.scatter(X[:,1], y, color = 'red', marker = '+', label = 'Training Data X3')
plt.plot(X[:,1], X.dot(theta), color = 'green', label = 'Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('X of data set')
plt.ylabel('y of data set')
plt.title('Linear Regression Fit')
plt.legend()

plt.plot(range(1, iterations  + 1), cost_history, color = 'blue')
plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

X = np.hstack((X_0, X_1, X_2, X_3))
X[:10]

theta = np.zeros(4)
theta

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)

plt.plot(range(1, iterations  + 1), cost_history, color = 'blue')
plt.rcParams["figure.figsize"] = (10,6)
plt.grid()
plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

